{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044c4f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch as T\n",
    "# each thread gets its own individual actor critic \n",
    "from a3c import ActorCritic\n",
    "from icm import ICM\n",
    "from memory import Memory\n",
    "from utils import plot_learning_curve\n",
    "\n",
    "def worker(name, input_shape, n_actions, global_agent, global_icm,\n",
    "           optimizer, icm_optimizer, env_id, n_threads, icm=False):\n",
    "    T_MAX = 20\n",
    "\n",
    "    local_agent = ActorCritic(input_shape, n_actions)\n",
    "\n",
    "    if icm:\n",
    "        local_icm = ICM(input_shape, n_actions)\n",
    "        # just a string for printing debug information to the terminal && saving our plot \n",
    "        algo = 'ICM'\n",
    "    else:\n",
    "        intrinsic_reward = T.zeros(1)\n",
    "        algo = 'A3C'\n",
    "    # each agent gets each own memory \n",
    "    memory = Memory()\n",
    "    # each own environment\n",
    "    env = gym.make(env_id)\n",
    "    # how many time steps we have, the episode , the score, the average score\n",
    "    t_steps, max_eps, episode, scores, avg_score = 0, 1000, 0, [], 0\n",
    "\n",
    "    while episode < max_eps:\n",
    "        obs = env.reset()\n",
    "        # make your hidden state for the actor critic a3c\n",
    "        hx = T.zeros(1, 256)\n",
    "        # we need a score, a terminal flag and the number of steps taken withing the episode\n",
    "        # every 20 steps in an episode we want to excecute the learning function\n",
    "        score, done, ep_steps = 0, False, 0\n",
    "        while not done:\n",
    "            state = T.tensor([obs], dtype=T.float)\n",
    "            # feed forward our state and our hidden state to the local agent to get the action we want to take, value for that state, log_prob for that action\n",
    "            action, value, log_prob, hx = local_agent(state, hx)\n",
    "            # take your action\n",
    "            obs_, reward, done, info = env.step(action)\n",
    "            # increment total steps, episode steps, increase your score\n",
    "            t_steps += 1\n",
    "            ep_steps += 1\n",
    "            score += reward\n",
    "            reward = 0  # turn off extrinsic rewards\n",
    "            memory.remember(obs, action, reward, obs_, value, log_prob)\n",
    "            obs = obs_\n",
    "            # LEARNING\n",
    "            # every 20 steps or when the game is done\n",
    "            if ep_steps % T_MAX == 0 or done:\n",
    "                states, actions, rewards, new_states, values, log_probs = \\\n",
    "                        memory.sample_memory()\n",
    "                # If we are doing icm them we want to calculate our loss according to icm\n",
    "                if icm:\n",
    "                    intrinsic_reward, L_I, L_F = \\\n",
    "                            local_icm.calc_loss(states, new_states, actions)\n",
    "                # loss according to our a3c agent\n",
    "                loss = local_agent.calc_loss(obs, hx, done, rewards, values,\n",
    "                                             log_probs, intrinsic_reward)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                # detach hx bc it gives us a reward trying to backpropagate through the graph twice \n",
    "                hx = hx.detach_()\n",
    "                if icm:\n",
    "                    icm_optimizer.zero_grad()\n",
    "                   # backpropagate your loss \n",
    "                    (L_I + L_F).backward()\n",
    "                # backpropagate your loss from your a3c\n",
    "                loss.backward()\n",
    "                T.nn.utils.clip_grad_norm_(local_agent.parameters(), 40)\n",
    "\n",
    "                for local_param, global_param in zip(\n",
    "                                        local_agent.parameters(),\n",
    "                                        global_agent.parameters()):\n",
    "                    global_param._grad = local_param.grad\n",
    "                optimizer.step()\n",
    "                # take our gradients from our local agent and upload them to the global agent\n",
    "                local_agent.load_state_dict(global_agent.state_dict())\n",
    "\n",
    "                if icm:\n",
    "                    for local_param, global_param in zip(\n",
    "                                            local_icm.parameters(),\n",
    "                                            global_icm.parameters()):\n",
    "                        global_param._grad = local_param.grad\n",
    "                    icm_optimizer.step()\n",
    "                    local_icm.load_state_dict(global_icm.state_dict())\n",
    "                    # clear memory at the end of the episode\n",
    "                memory.clear_memory()\n",
    "        # at the end of each episode\n",
    "        if name == '1':\n",
    "            # append our score to our scores array\n",
    "            scores.append(score)\n",
    "            avg_score = np.mean(scores[-100:])\n",
    "            print('{} episode {} thread {} of {} steps {:.2f}M score {:.2f} '\n",
    "                  'intrinsic_reward {:.2f} avg score (100) {:.1f}'.format(\n",
    "                      algo, episode, name, n_threads,\n",
    "                      t_steps/1e6, score,\n",
    "                      T.sum(intrinsic_reward),\n",
    "                      avg_score))\n",
    "        episode += 1\n",
    "    # at the end of all the episodes    \n",
    "    if name == '1':     \n",
    "        x = [z for z in range(episode)]\n",
    "        # for the first agent we want to plot our graph\n",
    "        fname = algo + '_CartPole_no_rewards.png'\n",
    "        plot_learning_curve(x, scores, fname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
