{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e1c4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# AC3 is the actual on-policy algorithm to generate the policy π\n",
    "# Used for environments with discrete action spaces --> AC3\n",
    "# This class is going to inherit from the nn module\n",
    "class ActorCritic(nn.Module):\n",
    "    #The __init__ method lets the class initialize the object’s attributes\n",
    "    # tau is the constant lamda from the paper\n",
    "    def __init__(self, input_dims, n_actions, gamma= 0.0, tau= 0.98):\n",
    "        # super() corresponds to nn.Module and it is running the initialisation for the nn.Module as well as (self)\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        # Our network will need an input layer which will take an input and translate that into 256\n",
    "        self.input = n.Linear(*input_dims, 256)\n",
    "        # A dense layer\n",
    "        self.dense = nn.Linear(256, 256)\n",
    "        # Lstm type layer\n",
    "        self.gru = nn.GRUCell(256, 256)\n",
    "        # Policy\n",
    "        self.pi = nn.Linear(256, n_actions)\n",
    "        # Value function\n",
    "        self.v = nn.Linear(256, 1)\n",
    "    \n",
    "    # It will take a state and a hidden state for our GRU as an input\n",
    "    def forward(self, state, hx):\n",
    "        x = F.relu(self.input(state))\n",
    "        x = F.relu(self.dense(x))\n",
    "        hx = self.gru(x, (hx))\n",
    "        \n",
    "        # Pass hidden state into our pi and v layer to get our logs for our policy and out value function\n",
    "        pi = self.pi(hx)\n",
    "        v = self.v(hx)\n",
    "        \n",
    "        # Choose action function/ Get the actual probability distribution\n",
    "        probs = T.softmax(pi, dim=1) # soft max activation on the first dimension\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        return action.numpy([0]), v, log_prob, hx\n",
    "    \n",
    "    # Functions to handle the calculation of the loss\n",
    "    # https://arxiv.org/pdf/1602.01783.pdf\n",
    "    def calc_R(self, done, rewards, values): # done/terminal flag, set of rewards, set of values--> stored in a list of tensors\n",
    "        # we want to convert this list of tensors to a single tensor and squeeze it because we dont want T time steps by 1\n",
    "        values = T.cat(values).squeeze()\n",
    "        # A3C must get triggered every T timestep or everytime an episode ends / we could have a batch of states or a single state\n",
    "        # if we have batch of states then the length of values.size is one\n",
    "        if len(values.size()) == 1: # batch of states\n",
    "            # last value of the value array \n",
    "            # multiplied by (1- int(done)) because the value of the terminal state is identically 0\n",
    "            R = values[-1] * (1- int(done))\n",
    "        elif len(values.size()) == 0: # single state\n",
    "            R = values*(1- int(done))\n",
    "        \n",
    "        # Calculate the returns at each time step of R sequence\n",
    "        batch_return = []\n",
    "        # Iterate backwards in our rewards\n",
    "        for reward in rewards[::-1]:\n",
    "            R = reward + self.gamma * R\n",
    "            batch_return.append(R)  \n",
    "            \n",
    "        batch_return.reverse() # reverse it\n",
    "        # convert it to a tensor \n",
    "        batch_return = T.tensor(batch_return, dtype = T.float).reshape(values.size())\n",
    "        return batch_return\n",
    "    # r_i_t --> intrisic reward\n",
    "    def calc_loss(seld, new_states, hx, done, rewards, values, log_probs, r_i_t = None):\n",
    "        # if we are supplying an intrinsic reward them we want to add the reward from ICM\n",
    "        if r_i_t is not None:\n",
    "            rewards += r_i_t.detach().numpy() # convert r_i_t to a numpy array because r_i_t is a tensor while rewards is a list of floating point values\n",
    "        returns = self.calc_R(done, rewards, values)\n",
    "        # calculate generalised advantage\n",
    "        # We need a value function for the state one step after our horizon\n",
    "        # get the first element because other elements that the forward function returns are not the value function (we want the element v )\n",
    "        next_v = T.zeros(1, 1) if done else self.forward(T.tensor([new_states],\n",
    "                                         dtype=T.float), hx)[1]\n",
    "        values.append( next_v.detach())\n",
    "        values = T.cat(values).squeeze()\n",
    "        log_probs = T.cat(log_probs) # concatinate -> cat\n",
    "        rewards = T.tensor(rewards)\n",
    "        #                   state of time at t+1  state of time at t\n",
    "        delta_t = rewards + self.gamma*values[1:] - values[:-1]\n",
    "        \n",
    "        n_steps = len(delta_t)\n",
    "        '''generalised advantage estimate : https://arxiv.org/pdf/1506.02438.pdf'''\n",
    "        # There is gonna be an advantage for each time step in the sequence\n",
    "        # So gae is gonna be a batch of states, T in length\n",
    "        # So we have an advantage for each timestep, which is proportional to a sum of all the rewards that follow\n",
    "        gae = np.zeros(n_steps)\n",
    "        # For each step in the sequence\n",
    "        for t in range(n_steps):\n",
    "            # for from that step onwards to the end\n",
    "            for k in range(0, n_steps-t):\n",
    "                temp = (self.gamma*self.tau)**k*delta_t[t+k]\n",
    "                gae[t] += temp\n",
    "        gae = T.tensor(gae, dtype=T.float)\n",
    "        \n",
    "        # Calculate losses \n",
    "        actor_losses = -(log_probs*gae).sum()\n",
    "        entropy_loss = (-log_probs*T.exp(log_probs)).sum()\n",
    "        # bc we stuck with the value function for the state that occurs at T timestep\n",
    "        critic_loss = F.mse_loss(values[:-1].squeeze(), returns) # mean squared error\n",
    "        total_loss = actor_loss + critic_loss - 0.01*entropy_loss\n",
    "       \n",
    "        return total_loss\n",
    "                \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4772e69d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
