{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecc1df27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "'''In the inverse model you want to predict the action the agent took to cause this state to transition from time t to t+1\n",
    "So you are comparing an integer vs an actula label/ the actual action the agent took\n",
    "Multi-class classification problem\n",
    "This is a cross entropy loss between the predicted action and the actual action the agent took'''\n",
    "\"The loss for the forward model is the mse between the predicted state at time t+1 and the actua state at time t+1  \"\n",
    "\"So we have two losses : one that comes from the inverse model and one that comes from the forward model \"\n",
    "class ICM(nn.Module):\n",
    "    def __init__(self, input_dims, n_actions=2, alpha=1, beta=0.2):\n",
    "        super(ICM, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        # hard-coded for the cartpole environment\n",
    "        # Inverse takes two successive states and tries to predict one action\n",
    "        # The input is gonna be 4*2 bc the input vector from the * environment has 4 elements, and we are gonna have two of them \n",
    "        self.inverse = nn.Linear(4*2, 256) # This is gonna feed to a layer of 256\n",
    "        # The logits of our policy is what we want to predict \n",
    "        self.pi_logits = nn.LInear(256, n_actions)\n",
    "        # Then the forward model takes a state and an action and asks what is the resulting state\n",
    "        self.dense1 = nn.Linear(4+1) # so inputs are 4+1 : 4 for the environment and 1 for one action --> it fits into a layer of 256 units\n",
    "        # state that we output\n",
    "        self.new_state = nn.Linear(256, 4) # if not hard-coded 4 = *input_dims\n",
    "        \n",
    "        device = T.device('cpu')\n",
    "        self.to(device)\n",
    "        \n",
    "        def forward(self, state, new_state, action):\n",
    "            \"We have to concatenate a state and action and pass it through the inverse layer \"\n",
    "            \"and activate it with an elu activation--> exponential linear\"\n",
    "            # Create inverse layer\n",
    "            inverse = F.elu(self.inverse(T.cat([state, new_state], dim=1)))\n",
    "            pi_logits = self.pi_logits(inverse)\n",
    "            \n",
    "            # Forward model\n",
    "            # from [T] to [T,1]\n",
    "            action = action.reshape((action.size()[0], 1))\n",
    "            forward_input = T.cat([state, action], dim=1)\n",
    "            # Activate the forward input and get a new state on the other end\n",
    "            dense = F.elu(self.dense1(forward_input))\n",
    "            state_ = self.new_state\n",
    "            \n",
    "            return pi_logits, state_\n",
    "        \n",
    "        \n",
    "        def calc_loss(self, state, new_state, action):\n",
    "            state = T.tensor(state, dtype=T.float)\n",
    "            action = T.tensor(action, dtype=T.float)\n",
    "            new_state = T.tensor(new_state, dtype=T.float)\n",
    "            # feed/pass state, new_state , action through our network\n",
    "            pi_logits, state_ = self.forward(state, new_state, action)\n",
    "            \"Our inverse loss is a cross entropy loss because this will generally have more than two actions\"\n",
    "            inverse_loss = nn.CrossEntropyLoss()\n",
    "            L_I = (1-self.beta)*inverse_loss(pi_logits, action.to(T.long))\n",
    "            \"Forward loss is mse between predicted new state and actual new state\"\n",
    "            forward_loss = nn.MSELoss()\n",
    "            L_F = self.beta*forward_loss(state_, new_state)\n",
    "            \"dim=1 for mean(dim=1) is very important. If you take that out it will take the mean across all dimensions and you just get a single number, which is not useful\"\n",
    "            \"because the curiosity reward is associated with each state, so you have to take the mean across that first dimension which is the number of states\"\n",
    "            intrinsic_reward = self.alpha*((state_ - new_state).pow(2)).mean(dim=1)\n",
    "            return intrinsic_reward, L_I, L_F \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c59e2c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
